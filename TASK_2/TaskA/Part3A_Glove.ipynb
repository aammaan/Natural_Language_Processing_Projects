{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NER_train.json') as file:\n",
    "    data_train_bio = json.load(file)\n",
    "with open('NER_val.json') as file:\n",
    "    data_val_bio = json.load(file) \n",
    "\n",
    "train_texts = [data_train_bio[entry]['text'] for entry in data_train_bio]\n",
    "train_tag_seqs = [data_train_bio[entry]['labels'] for entry in data_train_bio]\n",
    "\n",
    "val_texts = [data_train_bio[entry]['text'] for entry in data_val_bio]\n",
    "val_tag_seqs = [data_train_bio[entry]['labels'] for entry in data_val_bio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "OUT_OF_VOCAB = '<OOV>'\n",
    "glove_model_path = '../glove.42B.300d.txt'\n",
    "\n",
    "# Load GloVe word embeddings\n",
    "def load_glove_model(glove_model_path):\n",
    "    print(\"Loading GloVe word embeddings...\")\n",
    "    with open(glove_model_path, 'r', encoding='utf-8') as f:\n",
    "        embeddings_index = {}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "word2vec= load_glove_model(glove_model_path)\n",
    "\n",
    "word_map = {OUT_OF_VOCAB: 0}\n",
    "label_list = ['B_GPE', 'B_PETITIONER', 'B_ORG', 'I_CASE_NUMBER', 'I_ORG', 'I_STATUTE', 'B_RESPONDENT', 'I_JUDGE', 'I_GPE', 'B_COURT', 'I_DATE', 'B_OTHER_PERSON', 'B_PRECEDENT', 'B_JUDGE', 'O', 'I_PETITIONER', 'I_OTHER_PERSON', 'B_STATUTE', 'I_RESPONDENT', 'B_WITNESS', 'B_CASE_NUMBER', 'I_COURT', 'B_DATE', 'I_WITNESS', 'I_PROVISION', 'I_PRECEDENT', 'B_PROVISION', START_TAG, STOP_TAG, OUT_OF_VOCAB]\n",
    "label_map = {label_list[i]:i for i in range(len(label_list))}\n",
    "\n",
    "for sentence in train_texts:\n",
    "    for token in sentence:\n",
    "        if((token not in word_map) and (token in word2vec)):\n",
    "            word_map[token] = len(word_map)\n",
    "\n",
    "vocab_size = len(word_map)\n",
    "input_size = 300\n",
    "hidden_size = 32\n",
    "output_size = 5\n",
    "num_epochs = 15\n",
    "\n",
    "word_embeddings = np.zeros((vocab_size, input_size))\n",
    "for token, index in word_map.items():\n",
    "    if(token!=OUT_OF_VOCAB):\n",
    "        word_embeddings[index] = word2vec[token]\n",
    "\n",
    "word_embeddings[word_map[OUT_OF_VOCAB]] = np.zeros(300)\n",
    "word_embeddings = torch.tensor(word_embeddings.astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item() \n",
    "\n",
    "def prepare_sentence(tag_to_ix, sentence, tags):\n",
    "    vectorized_sentence = [word_map[token] if token in word_map else word_map[OUT_OF_VOCAB] for token in sentence]\n",
    "    vectorized_tag_seq = [tag_to_ix[tag] for tag in tags]\n",
    "    return torch.tensor(vectorized_sentence), torch.tensor(vectorized_tag_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, tag_to_ix):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.word_map = word_map\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embeddings)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -1000000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -1000000\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(2, 1, self.hidden_dim // 2), torch.zeros(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.full((1, self.tagset_size), -1000000.)\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        for feat in feats:\n",
    "            alphas_t = []  \n",
    "            for next_tag in range(self.tagset_size):\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()label_map  = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4})\n",
    "        return x\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    \n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        init_vvars = torch.full((1, self.tagset_size), -1000000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  \n",
    "            viterbivars_t = [] \n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG] \n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"nlp_ass2A\", \n",
    "    name=f\"BiLSTM_Glove\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, train_sentence, train_tags, val_sentence, val_tags, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(len(train_sentence)):\n",
    "        \n",
    "        raw_sentence, raw_tag_seq = train_sentence[i], train_tags[i]\n",
    "        vectorized_sentence, vectorized_tag_seq = prepare_sentence(label_map, raw_sentence, raw_tag_seq)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.neg_log_likelihood(vectorized_sentence, vectorized_tag_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss = 0.0\n",
    "    tag_seqs = []\n",
    "    tag_seqs_pred = [] \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(train_sentence)):\n",
    "            \n",
    "            raw_sentence, raw_tag_seq = train_sentence[i], train_tags[i]\n",
    "            vectorized_sentence, vectorized_tag_seq = prepare_sentence(label_map, raw_sentence, raw_tag_seq)\n",
    "            \n",
    "            score, tag_seq_pred = model(vectorized_sentence)\n",
    "            \n",
    "            loss += model.neg_log_likelihood(vectorized_sentence, vectorized_tag_seq)\n",
    "            tag_seqs.extend(vectorized_tag_seq.tolist())\n",
    "            tag_seqs_pred.extend(tag_seq_pred)\n",
    "    \n",
    "    avg_loss = loss/len(train_sentence)\n",
    "    f1 = f1_score(tag_seqs, tag_seqs_pred, average=\"macro\")\n",
    "    print(\"Training Loss\", avg_loss, \"F1 Score\", f1)\n",
    "\n",
    "    loss = 0.0\n",
    "    tag_seqs = []\n",
    "    tag_seqs_pred = [] \n",
    "    log_metric = {\"Epoch\": epoch+1, \"Training Loss\": avg_loss, \"Training F1 Score\": f1} \n",
    "    wandb.log(log_metric)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(val_sentence)):\n",
    "            \n",
    "            raw_sentence, raw_tag_seq = val_sentence[i], val_tags[i]\n",
    "            vectorized_sentence, vectorized_tag_seq = prepare_sentence(label_map, raw_sentence, raw_tag_seq)\n",
    "            \n",
    "            score, tag_seq_pred = model(vectorized_sentence)\n",
    "            \n",
    "            loss += model.neg_log_likelihood(vectorized_sentence, vectorized_tag_seq)\n",
    "            tag_seqs.extend(vectorized_tag_seq.tolist())\n",
    "            tag_seqs_pred.extend(tag_seq_pred)\n",
    "    \n",
    "    avg_loss = loss/len(val_sentence)\n",
    "    f1 = f1_score(tag_seqs, tag_seqs_pred, average=\"macro\")\n",
    "    log_metric = {\"Validation Loss\": avg_loss, \"Validation F1 Score\": f1}\n",
    "    wandb.log(log_metric)\n",
    "    print(\"Validation Loss\", avg_loss, \"F1 Score\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(input_size, hidden_size, label_map)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    evaluate(model, train_texts, train_tag_seqs, val_texts, val_tag_seqs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NER_test.json') as file:\n",
    "    data_val_bio = json.load(file) \n",
    "\n",
    "test_sentence = [data_train_bio[entry]['text'] for entry in data_val_bio]\n",
    "test_tags = [data_train_bio[entry]['labels'] for entry in data_val_bio] \n",
    "\n",
    "loss = 0.0\n",
    "tag_seqs = []\n",
    "tag_seqs_pred = [] \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_sentence)):\n",
    "        \n",
    "        raw_sentence, raw_tag_seq = test_sentence[i], test_tags[i]\n",
    "        vectorized_sentence, vectorized_tag_seq = prepare_sentence(label_map, raw_sentence, raw_tag_seq)\n",
    "        \n",
    "        score, tag_seq_pred = model(vectorized_sentence)\n",
    "        \n",
    "        loss += model.neg_log_likelihood(vectorized_sentence, vectorized_tag_seq)\n",
    "        tag_seqs.extend(vectorized_tag_seq.tolist())\n",
    "        tag_seqs_pred.extend(tag_seq_pred)\n",
    "\n",
    "avg_loss = loss/len(test_sentence)\n",
    "f1 = f1_score(tag_seqs, tag_seqs_pred, average=\"macro\")\n",
    "print(\"Validation Loss\", avg_loss, \"F1 Score\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'t2_glove_fasttext.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
