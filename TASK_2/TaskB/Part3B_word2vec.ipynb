{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bad1ab8c810>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ATE_train.json') as file:\n",
    "    data_train_bio = json.load(file)\n",
    "with open('ATE_val.json') as file:\n",
    "    data_val_bio = json.load(file) \n",
    "\n",
    "train_texts = [data_train_bio[entry]['text'] for entry in data_train_bio]\n",
    "train_tag_seqs = [data_train_bio[entry]['labels'] for entry in data_train_bio]\n",
    "\n",
    "val_texts = [data_val_bio[entry]['text'] for entry in data_val_bio]\n",
    "val_tag_seqs = [data_val_bio[entry]['labels'] for entry in data_val_bio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "OUT_OF_VOCAB = '<OOV>'\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "word_map = {OUT_OF_VOCAB: 0}\n",
    "label_map  = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "for sentence in train_texts:\n",
    "    for token in sentence:\n",
    "        if((token not in word_map) and (token in word2vec)):\n",
    "            word_map[token] = len(word_map)\n",
    "\n",
    "vocab_size = len(word_map)\n",
    "input_size = 300\n",
    "hidden_size = 32\n",
    "output_size = 5\n",
    "num_epochs = 15\n",
    "\n",
    "word_embeddings = np.zeros((vocab_size, input_size))\n",
    "for token, index in word_map.items():\n",
    "    if(token!=OUT_OF_VOCAB):\n",
    "        word_embeddings[index] = word2vec[token]\n",
    "\n",
    "word_embeddings[word_map[OUT_OF_VOCAB]] = np.zeros(300)\n",
    "word_embeddings = torch.tensor(word_embeddings.astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item() \n",
    "\n",
    "def prepare_sentence(tag_to_ix, sentence, tags):\n",
    "    vectorized_sentence = [word_map[token] if token in word_map else word_map[OUT_OF_VOCAB] for token in sentence]\n",
    "    vectorized_tag_seq = [tag_to_ix[tag] for tag in tags]\n",
    "    return torch.tensor(vectorized_sentence), torch.tensor(vectorized_tag_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, tag_to_ix):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.word_map = word_map\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embeddings)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -1000000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -1000000\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(2, 1, self.hidden_dim // 2), torch.zeros(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.full((1, self.tagset_size), -1000000.)\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        for feat in feats:\n",
    "            alphas_t = []  \n",
    "            for next_tag in range(self.tagset_size):\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        x = self.embedding(sentence).view(len(sentence), 1, -1)\n",
    "        x, self.hidden = self.lstm(x, self.hidden)\n",
    "        x = x.view(len(sentence), self.hidden_dim)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    \n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        init_vvars = torch.full((1, self.tagset_size), -1000000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  \n",
    "            viterbivars_t = [] \n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG] \n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.login()\n",
    "# wandb.init(\n",
    "#     project=\"nlp_ass2B\", \n",
    "#     name=f\"BiLSTM_Word2Vec\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, train_sentence, train_tags, val_sentence, val_tags, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(len(train_sentence)):\n",
    "        \n",
    "        raw_sentence, raw_tag_seq = train_sentence[i], train_tags[i]\n",
    "        vectorized_sentence, vectorized_tag_seq = prepare_sentence(label_map, raw_sentence, raw_tag_seq)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.neg_log_likelihood(vectorized_sentence, vectorized_tag_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss = 0.0\n",
    "    tag_seqs = []\n",
    "    tag_seqs_pred = [] \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(train_sentence)):\n",
    "            \n",
    "            raw_sentence, raw_tag_seq = train_sentence[i], train_tags[i]\n",
    "            vectorized_sentence, vectorized_tag_seq = prepare_sentence(label_map, raw_sentence, raw_tag_seq)\n",
    "            \n",
    "            score, tag_seq_pred = model(vectorized_sentence)\n",
    "            \n",
    "            loss += model.neg_log_likelihood(vectorized_sentence, vectorized_tag_seq)\n",
    "            tag_seqs.extend(vectorized_tag_seq.tolist())\n",
    "            tag_seqs_pred.extend(tag_seq_pred)\n",
    "    \n",
    "    avg_loss = loss/len(train_sentence)\n",
    "    f1 = f1_score(tag_seqs, tag_seqs_pred, average=\"macro\")\n",
    "    print(\"Training Loss\", avg_loss, \"F1 Score\", f1)\n",
    "\n",
    "    loss = 0.0\n",
    "    tag_seqs = []\n",
    "    tag_seqs_pred = [] \n",
    "    log_metric = {\"Epoch\": epoch+1, \"Training Loss\": avg_loss, \"Training F1 Score\": f1} \n",
    "    # wandb.log(log_metric)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(val_sentence)):\n",
    "            \n",
    "            raw_sentence, raw_tag_seq = val_sentence[i], val_tags[i]\n",
    "            vectorized_sentence, vectorized_tag_seq = prepare_sentence(label_map, raw_sentence, raw_tag_seq)\n",
    "            \n",
    "            score, tag_seq_pred = model(vectorized_sentence)\n",
    "            \n",
    "            loss += model.neg_log_likelihood(vectorized_sentence, vectorized_tag_seq)\n",
    "            tag_seqs.extend(vectorized_tag_seq.tolist())\n",
    "            tag_seqs_pred.extend(tag_seq_pred)\n",
    "    \n",
    "    avg_loss = loss/len(val_sentence)\n",
    "    f1 = f1_score(tag_seqs, tag_seqs_pred, average=\"macro\")\n",
    "    log_metric = {\"Validation Loss\": avg_loss, \"Validation F1 Score\": f1}\n",
    "    # wandb.log(log_metric)\n",
    "    print(\"Validation Loss\", avg_loss, \"F1 Score\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss tensor([5.4380]) F1 Score 0.48851928714228715\n",
      "Validation Loss tensor([5.3672]) F1 Score 0.4871601572773689\n",
      "Training Loss tensor([3.7752]) F1 Score 0.5944151765626381\n",
      "Validation Loss tensor([4.0697]) F1 Score 0.5644564394830209\n",
      "Training Loss tensor([2.7728]) F1 Score 0.7579654060698889\n",
      "Validation Loss tensor([3.3201]) F1 Score 0.6856774676369577\n",
      "Training Loss tensor([2.1940]) F1 Score 0.8166953176552066\n",
      "Validation Loss tensor([2.9444]) F1 Score 0.721681197864282\n",
      "Training Loss tensor([1.8170]) F1 Score 0.8520801354639347\n",
      "Validation Loss tensor([2.7441]) F1 Score 0.758889761905233\n",
      "Training Loss tensor([1.5385]) F1 Score 0.8732942380881412\n",
      "Validation Loss tensor([2.6337]) F1 Score 0.7705653705323647\n",
      "Training Loss tensor([1.3125]) F1 Score 0.8933565938985261\n",
      "Validation Loss tensor([2.5829]) F1 Score 0.7699595902291652\n",
      "Training Loss tensor([1.1197]) F1 Score 0.9138341885270383\n",
      "Validation Loss tensor([2.5708]) F1 Score 0.7718569400842211\n",
      "Training Loss tensor([0.9510]) F1 Score 0.9305948433823925\n",
      "Validation Loss tensor([2.5949]) F1 Score 0.7902419007068309\n",
      "Training Loss tensor([0.8050]) F1 Score 0.9466486608495308\n",
      "Validation Loss tensor([2.6667]) F1 Score 0.7989325808821041\n",
      "Training Loss tensor([0.6791]) F1 Score 0.9604423965562123\n",
      "Validation Loss tensor([2.7675]) F1 Score 0.8037816532265923\n",
      "Training Loss tensor([0.5689]) F1 Score 0.9695586476796603\n",
      "Validation Loss tensor([2.8961]) F1 Score 0.7822271121728193\n",
      "Training Loss tensor([0.4752]) F1 Score 0.9756001577358084\n",
      "Validation Loss tensor([3.0337]) F1 Score 0.7835894743872398\n",
      "Training Loss tensor([0.3871]) F1 Score 0.9789444226368915\n",
      "Validation Loss tensor([3.2074]) F1 Score 0.7780026113296854\n",
      "Training Loss tensor([0.3035]) F1 Score 0.9816556507424535\n",
      "Validation Loss tensor([3.4826]) F1 Score 0.7679610681997039\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_CRF(input_size, hidden_size, label_map)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    evaluate(model, train_texts, train_tag_seqs, val_texts, val_tag_seqs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss tensor([3.8585]) F1 Score 0.7583996808462352\n"
     ]
    }
   ],
   "source": [
    "with open('ATE_test.json') as file:\n",
    "    data_val_bio = json.load(file) \n",
    "\n",
    "test_sentence = [data_val_bio[entry]['text'] for entry in data_val_bio]\n",
    "test_tags = [data_val_bio[entry]['labels'] for entry in data_val_bio] \n",
    "\n",
    "loss = 0.0\n",
    "tag_seqs = []\n",
    "tag_seqs_pred = [] \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_sentence)):\n",
    "        \n",
    "        raw_sentence, raw_tag_seq = test_sentence[i], test_tags[i]\n",
    "        vectorized_sentence, vectorized_tag_seq = prepare_sentence(label_map, raw_sentence, raw_tag_seq)\n",
    "        \n",
    "        score, tag_seq_pred = model(vectorized_sentence)\n",
    "        \n",
    "        loss += model.neg_log_likelihood(vectorized_sentence, vectorized_tag_seq)\n",
    "        tag_seqs.extend(vectorized_tag_seq.tolist())\n",
    "        tag_seqs_pred.extend(tag_seq_pred)\n",
    "\n",
    "avg_loss = loss/len(test_sentence)\n",
    "f1 = f1_score(tag_seqs, tag_seqs_pred, average=\"macro\")\n",
    "print(\"Validation Loss\", avg_loss, \"F1 Score\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'t2_bilstm_word2vec.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
