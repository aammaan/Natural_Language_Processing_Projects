{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/input/nlp-ass4-dataset/val_file.json', 'r') as val_file:\n",
    "    val_data = json.load(val_file)\n",
    "test_texts = [item[\"utterances\"] for item in val_data]\n",
    "test_labels = [item[\"emotions\"] for item in val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {'anger'  :1,\n",
    "               'joy'    :2,\n",
    "               'fear'   :3,\n",
    "               'disgust':4,\n",
    "               'neutral':5,\n",
    "               'surprise':6,\n",
    "               'sadness':7\n",
    "              }\n",
    "\n",
    "inverse_dict = {}\n",
    "for i in labels_dict:\n",
    "    inverse_dict[labels_dict[i]] = i\n",
    "inverse_dict[0] = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_test_inputs = []\n",
    "converted_test_labels = []\n",
    "\n",
    "for i in test_labels:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(labels_dict[j])\n",
    "    converted_test_labels.append(temp)\n",
    "\n",
    "for i, j in enumerate(test_texts):\n",
    "    temp_len = len(test_texts[i])\n",
    "    if(temp_len<25):\n",
    "        for lol in range(25-temp_len):\n",
    "            test_texts[i].append(\"\")\n",
    "            converted_test_labels[i].append(0)\n",
    "            \n",
    "    inputs = tokenizer(test_texts[i], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    converted_test_inputs.append(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc2 = nn.Linear(hidden_size//2, hidden_size//4)\n",
    "        self.fc3 = nn.Linear(hidden_size//4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "input_size = 768\n",
    "hidden_size = 256\n",
    "output_size = 8\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load('/kaggle/input/gru/pytorch/lstm_nlp4/1/lstm.pth'))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "text = [\"You-you\\u0085you had sex with Ursula?!\",\"Uh, a little bit. She-she-she walked in and I thought she was you and I kissed her and\",\"You didn't notice she was wearing different clothes?!\",\"Well I was just so excited to see you.\",\"Oh. Ew! Ew! Ew! Ugh! Y'know what? This is too weird.\"]\n",
    "temp_len = len(text)\n",
    "if(temp_len<25):\n",
    "    for lol in range(25-temp_len):\n",
    "        text.append(\"\")\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(**inputs)\n",
    "\n",
    "output = model(outputs[1])\n",
    "final_result = []\n",
    "for i in range(temp_len):\n",
    "    x = output[i]\n",
    "    probabilities = torch.nn.functional.softmax(x, dim=0)\n",
    "    predicted_class = torch.argmax(probabilities).item()\n",
    "    final_result.append(inverse_dict[predicted_class])\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc2 = nn.Linear(hidden_size//2, hidden_size//4)\n",
    "        self.fc3 = nn.Linear(hidden_size//4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "input_size = 768\n",
    "hidden_size = 256\n",
    "output_size = 8\n",
    "\n",
    "model = GRUModel(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load('/kaggle/input/gru/pytorch/gru_nlp4/1/gru.pth'))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "final_list = []\n",
    "for i in range(len(test_texts)):\n",
    "    inputs = converted_test_inputs[i]\n",
    "    labels = converted_test_labels[i]\n",
    "    label_list.append(labels)\n",
    "    zero_c = labels.count(0)\n",
    "    output = model(inputs.to(device))\n",
    "    temp_inputs = [x for x in test_texts[i] if x!=\"\"]\n",
    "    for j in range(len(inputs)+1):\n",
    "        x = output[j]\n",
    "        probabilities = torch.nn.functional.softmax(x, dim=0)\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "        final_list.append(predicted_class)\n",
    "#         print(predicted_class)\n",
    "print(len(label_list))\n",
    "print(len(final_list))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(label_list, final_list)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
